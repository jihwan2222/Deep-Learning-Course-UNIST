{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#2. Kaggle Dataset\n",
        "\n",
        "- Although simple datasets are provided by PyTorch, constructing own custom dataset is the most difficult and time-consuming part in deep learning.\n",
        "\n",
        "- Therefore, we will use *somewhat* raw data and train a model.\n",
        "\n",
        "- We will use [100 Sports Image Classification](https://www.kaggle.com/datasets/gpiosenka/sports-classification) dataset which contains 13,572 training data, 500 validation and 500 test data with the shape of [H, W, C] = [224, 244, 3] in jpeg format.\n",
        "\n",
        "- Since the structure of dataset is constructed as the subfolders of each class with image, there are two ways we can construct dataset as follows:\n",
        "    - Building custom dataset \n",
        "    - Use `torch.utils.data.DataFolder`\n",
        "\n",
        "`TODO: Add a brief explanation why we need to make custom dataset`"
      ],
      "metadata": {
        "id": "ri5WF2NSloxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "L35vj4snsfYG"
      },
      "outputs": [],
      "source": [
        "# Import libraries to use for Deep Learning \n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.io import read_image\n",
        "from torchsummary import summary\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os \n",
        "\n",
        "import cv2 as cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown && gdown 'https://drive.google.com/uc?id=1rctM1HDoc24XOcRzsYyTSavaFrvuoKZc' && unzip ./archive.zip -d ./sports"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76a_lqvlskhL",
        "outputId": "1ae087b6-9b4b-40e7-da60-7bec9926ee74"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rctM1HDoc24XOcRzsYyTSavaFrvuoKZc\n",
            "To: /content/archive.zip\n",
            "100% 500M/500M [00:01<00:00, 302MB/s]\n",
            "Archive:  ./archive.zip\n",
            "replace ./sports/EfficientNetB3-sports-0.97.h5? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Dataset can be constructed as follows:\n",
        "\n",
        "\n",
        "```\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    # Inherit torch.utils.data.Dataset class\n",
        "\n",
        "    def __init__(self,):\n",
        "        # Initialize the dataset (handling data paths, check input and target data, data augmentation, etc.)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of data or sample in dataset \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Return the input and target by index\n",
        "```\n",
        "\n",
        "- Further information, please refer [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n",
        "- Making custom dataset will be given as an assignment!"
      ],
      "metadata": {
        "id": "Sn0AW6IltkS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "### PLEASE WRITE YOUR CODE BELOW.\n",
        "\n",
        "d1 = pd.read_csv('/content/sports/sports.csv')\n",
        "sports_train = d1[d1['data set'] == 'train']\n",
        "sports_valid = d1[d1['data set'] == 'valid'].reset_index(drop=True)\n",
        "sports_test = d1[d1['data set'] == 'test'].reset_index(drop=True)\n",
        "\n",
        "class_dict = pd.read_csv('/content/sports/class_dict.csv')\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self,first):\n",
        "        # Initialize the dataset (handling data paths, check input and target data, data augmentation, etc.)\n",
        "        \n",
        "        self.first = first\n",
        "\n",
        "        dict1 = {}\n",
        "        for i in range(100):\n",
        "            dict1[class_dict['class'][i]] = class_dict['class_index'][i]\n",
        "\n",
        "        self.new_csv = []\n",
        "\n",
        "        if first == 'original':\n",
        "            self.sports = d1           \n",
        "        elif first ==  'train':\n",
        "            self.sports = sports_train\n",
        "        elif first ==  'valid':\n",
        "            self.sports = sports_valid\n",
        "        elif first ==  'test':\n",
        "            self.sports = sports_test\n",
        "\n",
        "        \n",
        "        leng = len(self.sports['filepaths'])\n",
        "        for i in range(leng):\n",
        "           self.new_csv.append((self.sports['filepaths'][i], dict1[self.sports['labels'][i]])) \n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of data or sample in dataset \n",
        "        return len(self.new_csv)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Return the input and target by index\n",
        "        data_tuple = self.new_csv[index]\n",
        "        img_path = data_tuple[0]\n",
        "        label = data_tuple[1]\n",
        "\n",
        "        image = cv2.imread('/content/sports/' + img_path)\n",
        "        image = torch.tensor(image, dtype = torch.float32).permute(2, 0, 1)\n",
        "        label = torch.tensor(label) \n",
        "        # image = cv2.imread(os.path.join('/content/sports/' , img_path))\n",
        "\n",
        "        return image, label\n",
        "        \n",
        "# Deep learning model이 학습이나 model 자체를 통과시키려면, data type이 tensor인 것도 중요하지만, float32\n",
        "### END OF THE CODE."
      ],
      "metadata": {
        "id": "tYcwXdDWsqpt"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(CustomDataset('original'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwuLS-wdMFC5",
        "outputId": "e836fe31-ab00-4f73-da9d-cdbb0f724590"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14572"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(CustomDataset('valid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I13GUtn5I4uT",
        "outputId": "7836e5d0-132e-4e99-96b7-4aaa475f783f"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(CustomDataset('test'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkrJfYmC0sNv",
        "outputId": "bab9fc18-a8fc-4e6c-aeee-8ca29e4b6c57"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### PLEASE WRITE YOUR CODE BELOW.\n",
        "\n",
        "train_dataset = CustomDataset('train')\n",
        "valid_dataset = CustomDataset('valid')\n",
        "test_dataset = CustomDataset('test')\n",
        "\n",
        "### YOU CAN USE ANY TRANSFORMS YOU WANT. MAKE IT RUNNABLE!\n",
        "\n",
        "### NOTE: Fixed errata - changed train_dataloader, valid_dataloader, test_dataloader to train_loader, valid_loader, test_loader by Seungwoo\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "### END OF THE CODE."
      ],
      "metadata": {
        "id": "j1QWFB9AgSQg"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### PLEASE WRITE YOUR CODE BELOW.\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=3,\n",
        "                               stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                   \n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3,\n",
        "                               stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                   \n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,\n",
        "                               stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                   \n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,\n",
        "                               stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                   \n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features = 128*14*14, out_features = 5012),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(5012, 100)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "### END OF THE CODE."
      ],
      "metadata": {
        "id": "Z6JGskVnfMwk"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### NOTE: Fixed errata - changed the below codes by Seungwoo\n",
        "### model = SimpleCNN(in_channels=3, num_classes=100).to(device)\n",
        "### summary(model, (3, 256, 256), device='cuda') \n",
        "\n",
        "model = SimpleCNN(in_channels=3, num_classes=100).cuda()\n",
        "summary(model, (3, 224, 224), device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdOYVs2v1zm_",
        "outputId": "fcfdaa9a-2250-466d-f4d1-4401616a2b6a"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 224, 224]             448\n",
            "       BatchNorm2d-2         [-1, 16, 224, 224]              32\n",
            "              ReLU-3         [-1, 16, 224, 224]               0\n",
            "           Dropout-4         [-1, 16, 224, 224]               0\n",
            "         AvgPool2d-5         [-1, 16, 112, 112]               0\n",
            "            Conv2d-6         [-1, 32, 112, 112]           4,640\n",
            "       BatchNorm2d-7         [-1, 32, 112, 112]              64\n",
            "              ReLU-8         [-1, 32, 112, 112]               0\n",
            "           Dropout-9         [-1, 32, 112, 112]               0\n",
            "        AvgPool2d-10           [-1, 32, 56, 56]               0\n",
            "           Conv2d-11           [-1, 64, 56, 56]          18,496\n",
            "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
            "             ReLU-13           [-1, 64, 56, 56]               0\n",
            "          Dropout-14           [-1, 64, 56, 56]               0\n",
            "        AvgPool2d-15           [-1, 64, 28, 28]               0\n",
            "           Conv2d-16          [-1, 128, 28, 28]          73,856\n",
            "      BatchNorm2d-17          [-1, 128, 28, 28]             256\n",
            "             ReLU-18          [-1, 128, 28, 28]               0\n",
            "          Dropout-19          [-1, 128, 28, 28]               0\n",
            "        AvgPool2d-20          [-1, 128, 14, 14]               0\n",
            "           Linear-21                 [-1, 5012]     125,746,068\n",
            "             ReLU-22                 [-1, 5012]               0\n",
            "           Linear-23                  [-1, 100]         501,300\n",
            "================================================================\n",
            "Total params: 126,345,288\n",
            "Trainable params: 126,345,288\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 48.89\n",
            "Params size (MB): 481.97\n",
            "Estimated Total Size (MB): 531.43\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-4\n",
        "epochs = 2\n",
        "\n",
        "model = SimpleCNN(in_channels=3, num_classes=100).cuda()\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr)"
      ],
      "metadata": {
        "id": "nbYHFXRaN6ol"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, data_loader, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for idx, batch in enumerate(data_loader):\n",
        "        img, target = batch[0].cuda(), batch[1].cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() \n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch + 1, idx * img.size(0), len(data_loader.dataset),\n",
        "                100. * idx * img.size(0) / len(data_loader.dataset), \n",
        "                loss.data))\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def validate(model, criterion, data_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(data_loader):\n",
        "            img, target = batch[0].cuda(), batch[1].cuda()\n",
        "\n",
        "            ### PLEASE WRITE YOUR CODE BELOW.\n",
        "\n",
        "            # Make a prediction\n",
        "            output = model(img)\n",
        "            # Calculate validation loss (although it is optional)\n",
        "            loss = criterion(output, target)\n",
        "            # Get the right prediction - make sure naming the prediction as 'predicted' \n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (predicted == target).sum().item()\n",
        "            \n",
        "            ### END OF THE CODE.\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (predicted == target).sum().item()\n",
        "\n",
        "        total_val_acc = val_acc / len(data_loader.dataset)\n",
        "        print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            val_loss / len(data_loader), val_acc, len(data_loader.dataset),\n",
        "            100. * total_val_acc))\n",
        "    \n",
        "    return total_val_acc"
      ],
      "metadata": {
        "id": "RBKWnFbjN5r2"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, criterion, data_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_acc = 0.0\n",
        "\n",
        "    for idx, batch in enumerate(data_loader):\n",
        "        img, target = batch[0].cuda(), batch[1].cuda()\n",
        "\n",
        "        output = model(img)\n",
        "        loss = criterion(output, target)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        test_loss += loss.item()\n",
        "        test_acc += (predicted == target).sum().item()\n",
        "\n",
        "    print('\\n Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss  / len(data_loader), test_acc, len(data_loader.dataset),\n",
        "        100. * test_acc / len(data_loader.dataset)))\n",
        "\n",
        "    return test_acc / len(data_loader.dataset)"
      ],
      "metadata": {
        "id": "Y8VjZwCxYk6W"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, optimizer, criterion, train_loader, epoch)\n",
        "    validation_accuracy = validate(model, criterion, valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1pgEEyTlK0R",
        "outputId": "a8e85ee4-b122-402c-f64e-1cc245dae4ba"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/13572 (0%)]\tLoss: 4.618934\n",
            "Train Epoch: 1 [640/13572 (5%)]\tLoss: 4.633618\n",
            "Train Epoch: 1 [1280/13572 (9%)]\tLoss: 4.290107\n",
            "Train Epoch: 1 [1920/13572 (14%)]\tLoss: 4.071785\n",
            "Train Epoch: 1 [2560/13572 (19%)]\tLoss: 4.105429\n",
            "Train Epoch: 1 [3200/13572 (24%)]\tLoss: 3.955539\n",
            "Train Epoch: 1 [3840/13572 (28%)]\tLoss: 3.655684\n",
            "Train Epoch: 1 [4480/13572 (33%)]\tLoss: 3.878688\n",
            "Train Epoch: 1 [5120/13572 (38%)]\tLoss: 3.461308\n",
            "Train Epoch: 1 [5760/13572 (42%)]\tLoss: 3.644401\n",
            "Train Epoch: 1 [6400/13572 (47%)]\tLoss: 3.794811\n",
            "Train Epoch: 1 [7040/13572 (52%)]\tLoss: 3.401733\n",
            "Train Epoch: 1 [7680/13572 (57%)]\tLoss: 3.409593\n",
            "Train Epoch: 1 [8320/13572 (61%)]\tLoss: 3.258068\n",
            "Train Epoch: 1 [8960/13572 (66%)]\tLoss: 3.067349\n",
            "Train Epoch: 1 [9600/13572 (71%)]\tLoss: 3.282923\n",
            "Train Epoch: 1 [10240/13572 (75%)]\tLoss: 3.119306\n",
            "Train Epoch: 1 [10880/13572 (80%)]\tLoss: 3.596621\n",
            "Train Epoch: 1 [11520/13572 (85%)]\tLoss: 3.027433\n",
            "Train Epoch: 1 [12160/13572 (90%)]\tLoss: 2.987044\n",
            "Train Epoch: 1 [12800/13572 (94%)]\tLoss: 2.835579\n",
            "Train Epoch: 1 [13440/13572 (99%)]\tLoss: 3.369706\n",
            "\n",
            "Validation set: Average loss: 5.9168, Accuracy: 252.0/500 (50%)\n",
            "\n",
            "Train Epoch: 2 [0/13572 (0%)]\tLoss: 2.699765\n",
            "Train Epoch: 2 [640/13572 (5%)]\tLoss: 2.814088\n",
            "Train Epoch: 2 [1280/13572 (9%)]\tLoss: 2.667222\n",
            "Train Epoch: 2 [1920/13572 (14%)]\tLoss: 3.103415\n",
            "Train Epoch: 2 [2560/13572 (19%)]\tLoss: 2.664124\n",
            "Train Epoch: 2 [3200/13572 (24%)]\tLoss: 2.460018\n",
            "Train Epoch: 2 [3840/13572 (28%)]\tLoss: 2.217792\n",
            "Train Epoch: 2 [4480/13572 (33%)]\tLoss: 2.623333\n",
            "Train Epoch: 2 [5120/13572 (38%)]\tLoss: 2.975414\n",
            "Train Epoch: 2 [5760/13572 (42%)]\tLoss: 2.222962\n",
            "Train Epoch: 2 [6400/13572 (47%)]\tLoss: 2.152067\n",
            "Train Epoch: 2 [7040/13572 (52%)]\tLoss: 2.134598\n",
            "Train Epoch: 2 [7680/13572 (57%)]\tLoss: 2.666355\n",
            "Train Epoch: 2 [8320/13572 (61%)]\tLoss: 2.468549\n",
            "Train Epoch: 2 [8960/13572 (66%)]\tLoss: 2.551336\n",
            "Train Epoch: 2 [9600/13572 (71%)]\tLoss: 2.451642\n",
            "Train Epoch: 2 [10240/13572 (75%)]\tLoss: 2.526533\n",
            "Train Epoch: 2 [10880/13572 (80%)]\tLoss: 2.921004\n",
            "Train Epoch: 2 [11520/13572 (85%)]\tLoss: 2.901466\n",
            "Train Epoch: 2 [12160/13572 (90%)]\tLoss: 2.557681\n",
            "Train Epoch: 2 [12800/13572 (94%)]\tLoss: 2.534824\n",
            "Train Epoch: 2 [13440/13572 (99%)]\tLoss: 2.210627\n",
            "\n",
            "Validation set: Average loss: 4.9979, Accuracy: 360.0/500 (72%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = test(model, criterion, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9kyS9xAj2uv",
        "outputId": "6a843135-67d4-4b62-a222-334cdb27a75b"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Test set: Average loss: 2.2934, Accuracy: 201.0/500 (40%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0][0].size())\n",
        "print(model(torch.rand(1, 3, 224, 224, device='cuda')).size())\n",
        "test(model, criterion, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k-m3p4kblcG",
        "outputId": "09953670-f4b6-424f-c0b6-dffca4512243"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "torch.Size([1, 100])\n",
            "\n",
            " Test set: Average loss: 2.2934, Accuracy: 201.0/500 (40%)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.402"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    }
  ]
}